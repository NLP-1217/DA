{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xuesiyuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer,one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import *\n",
    "from re import split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from collections import Counter\n",
    "from listofimg import listOfimg\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_traindata = '/home/xuesiyuan/pythonworkspace/DailyDialogue/dailydialog/dialogues_text.txt'\n",
    "path_trainlabel = '/home/xuesiyuan/pythonworkspace/DailyDialogue/dailydialog/dialogues_act.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    utterance = []\n",
    "    act = []\n",
    "    f1 = open(path_traindata,'r')\n",
    "    f2 = open(path_trainlabel,'r')\n",
    "    for item in f1.readlines():\n",
    "        i = item.strip('\\n')\n",
    "        s = i.split(' __eou__')\n",
    "        for item in s:\n",
    "            if item != '':\n",
    "                utterance.append(item)\n",
    "\n",
    "    for item in f2.readlines():\n",
    "        i = item.strip('\\n')\n",
    "        split = i.split()\n",
    "        for i in split:\n",
    "            act.append(i)\n",
    "    return utterance,act\n",
    "\n",
    "def get_data(act_tag,utterance):\n",
    "    act_dic = dict() #lenth 43\n",
    "    cc = list(zip(act_tag,utterance))\n",
    "    for i in range(len(act_tag)):\n",
    "        if act_tag[i] not in act_dic.keys():\n",
    "            act_dic[act_tag[i]] = [cc[i][1]]\n",
    "        if act_tag[i] in act_dic.keys():\n",
    "            act_dic[act_tag[i]].append(cc[i][1])\n",
    "\n",
    "    actdict_list = []\n",
    "    label_list = []\n",
    "    for key,values in act_dic.items():\n",
    "        actdict_list.append(values)\n",
    "        label_list.append(key)\n",
    "\n",
    "    #padding\n",
    "    flatten = sum(actdict_list,[])\n",
    "    tokenizer = Tokenizer(filters='\\n')\n",
    "    tokenizer.fit_on_texts(flatten)\n",
    "    flatten = tokenizer.texts_to_sequences(flatten)\n",
    "    padding = pad_sequences(flatten)   \n",
    "    \n",
    "    data0 = []\n",
    "    count = 0\n",
    "    for i in actdict_list:\n",
    "        l = len(i)\n",
    "        data0.append(padding[count:count+l].tolist())\n",
    "        count += l\n",
    "    return (data0,label_list)\n",
    "\n",
    "\n",
    "def split(data): #先在整个数据集上随机抽取出8/2用作训练和测试\n",
    "    random.seed(1)\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in data:\n",
    "        a = int(len(i)*0.6)\n",
    "        random.shuffle(i)\n",
    "        train.append(i[:a])\n",
    "        test.append(i[a:])\n",
    "    return (train,test)\n",
    "\n",
    "\n",
    "def load_data_label(train,label_list):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i,j in zip(train,label_list):\n",
    "#         print (i,j)\n",
    "        for ii in i:\n",
    "            X.append(ii)\n",
    "            y.append(j)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return (X,y)\n",
    "   \n",
    "    \n",
    "def process_word(utterance):\n",
    "    ##vocab\n",
    "    tokenizer = Tokenizer(filters='\\n')\n",
    "    tokenizer.fit_on_texts(utterance)\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "#     input_utt_sequence = tokenizer.texts_to_sequences(anchor_utt)\n",
    "#     input_anchor_utt = pad_sequences(input_utt_sequence, maxlen = input_max_length)\n",
    "    print('--------------build embeding index---------------')\n",
    "    embeddings_index = dict()\n",
    "    f = open('/home/xuesiyuan/pythonworkspace/swda/glove.6B.50d.txt')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))    \n",
    "    embedding_matrix = np.zeros((vocab_size+1, 50))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return  vocab_size,embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_1D_bn(x, filters, kernel_size, stride=1, dilation_rate=1, padding='same', activation='relu'):\n",
    "    x = Conv1D(filters=filters, kernel_size=kernel_size, strides=stride,dilation_rate=dilation_rate, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if activation is not None:\n",
    "        activation_x = Activation(activation)(x)\n",
    "    return activation_x\n",
    "    \n",
    "def inceptionRes1D(input):\n",
    "    x0 = Activation('relu')(input)\n",
    "    x00 = conv_1D_bn(x0, 128, 1)\n",
    "    x1 = conv_1D_bn(x0, 32, 1)#1gram\n",
    "    x2 = conv_1D_bn(x0, 32, 2)#2gram\n",
    "    x3 = conv_1D_bn(x0, 32, 1, dilation_rate=2)\n",
    "    x4 = conv_1D_bn(x0, 32, 2, dilation_rate=2)\n",
    "    x11 = Concatenate()([x1,x2])\n",
    "    x11 = conv_1D_bn(x11, 128, 1)\n",
    "    x_con = Add()([x00, x11])\n",
    "    return(x_con)\n",
    "\n",
    "\n",
    "\n",
    "def attention_3d_block(inputs): #input 4\n",
    "    a = Dense(1, activation='tanh')(inputs)\n",
    "    a = Flatten()(a)\n",
    "    a = RepeatVector(128)(a)\n",
    "    a = Dense(input_max_length, activation='softmax')(a)\n",
    "    output_attention_mul = dot([a, inputs],axes = [2,1], name='attention_mul')\n",
    "    return output_attention_mul\n",
    "  \n",
    "    \n",
    "    \n",
    "def train_model():\n",
    "    \n",
    "    utt_input = Input(shape = (input_max_length,))\n",
    "    input_em = Embedding(input_dim = vocab_size+1, \n",
    "                     output_dim = 50, \n",
    "                     weights = [embedding_matrix], \n",
    "                     input_length = input_max_length, \n",
    "                     trainable = False)(utt_input)\n",
    "    cnn_encode = inceptionRes1D(input_em)\n",
    "    dropout1 = Dropout(0.5)(cnn_encode)\n",
    "    att_out = attention_3d_block(dropout1)\n",
    "    gru_encode = GRU(hidden_dim, name='lstm',return_sequences=True,recurrent_dropout=0.5)(att_out)\n",
    "    flatten = Flatten()(gru_encode)\n",
    "    utt_output = Dense(128, activation='sigmoid')(flatten)\n",
    "    utterance_model = Model(inputs = utt_input, outputs = utt_output,name = 'utterance_model')\n",
    "    utterance_model.summary()    \n",
    "  \n",
    "\n",
    "    #######################################################################################################\n",
    "\n",
    "    anchor_input = Input(shape = (input_max_length,),name='anchor_input') \n",
    "    positive_input = Input(shape = (input_max_length,),name='pos_input')\n",
    "    negative_input = Input(shape = (input_max_length,),name='neg_input')\n",
    "    \n",
    "    anchor_out = utterance_model(anchor_input)\n",
    "    positive_out = utterance_model(positive_input)\n",
    "    negative_out = utterance_model(negative_input)\n",
    "    \n",
    "    inputs = [anchor_input,positive_input,negative_input]\n",
    "    outputs = [anchor_out,positive_out,negative_out]\n",
    "    \n",
    "    triplet_model = Model(inputs, outputs)\n",
    "    triplet_model.add_loss(K.mean(triplet_loss(outputs)))\n",
    "#     triplet_model.summary()\n",
    "    \n",
    "    return utterance_model,triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(inputs, dist='sqeuclidean', margin='maxplus'):\n",
    "    head_input,positive_input,negative_input = inputs\n",
    "    positive_distance = K.square(head_input - positive_input)#平方和\n",
    "    negative_distance = K.square(head_input - negative_input)\n",
    "    \n",
    "    if dist == 'euclidean':\n",
    "        positive_distance = K.sqrt(K.sum(positive_distance, axis=-1, keepdims=True)) \n",
    "        negative_distance = K.sqrt(K.sum(negative_distance, axis=-1, keepdims=True))\n",
    "    elif dist == 'sqeuclidean':\n",
    "        positive_distance = K.sum(positive_distance, axis=-1, keepdims=True)\n",
    "        negative_distance = K.sum(negative_distance, axis=-1, keepdims=True)\n",
    "        \n",
    "    loss = positive_distance - negative_distance\n",
    "    \n",
    "    if margin == 'maxplus':\n",
    "        loss = K.maximum(0.0, 1 + loss) #margin = 1\n",
    "    elif margin == 'softplus':\n",
    "        loss = K.log(1 + K.exp(loss))\n",
    "        \n",
    "    return K.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data0,batch_size):  \n",
    "    count = 0\n",
    "    while True:\n",
    "        training = []\n",
    "        for _ in range(batch_size):\n",
    "            anchor_pos_pair = random.sample(data0,2)\n",
    "            anchor_pos = random.sample(anchor_pos_pair[0],2)\n",
    "            [neg] = random.sample(anchor_pos_pair[1],1)\n",
    "            anchor_pos.append(neg)\n",
    "            training.append(anchor_pos)\n",
    "        trans = np.array(training).transpose(1,0,2)\n",
    "        count +=1\n",
    "        yield ([trans[0],trans[1],trans[2]],None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_max_length= 278\n",
    "batch_size = 128\n",
    "hidden_dim = 128\n",
    "num_label = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------build embeding index---------------\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "utterance,act = load_data()\n",
    "data0,label_list = get_data(act,utterance)\n",
    "train,test = split(data0)\n",
    "vocab_size,embedding_matrix = process_word(utterance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29429\n",
      "9725\n",
      "17296\n",
      "46533\n"
     ]
    }
   ],
   "source": [
    "for i in data0:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So Dick , how about getting some coffee for tonight ?\n",
      "3\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 117, 25, 22, 3, 76, 1]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(utterance[2])\n",
    "print(act[2])\n",
    "print(data0[1][1])\n",
    "print(label_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_path ='./modelsave/1219_DailyDialogue_utterance.h5'\n",
    "triplet_path = './modelsave/1219_DailyDialogue_triplet.h5'\n",
    "result_path = './result_model/1219_DailyDialogue_model.h5'\n",
    "log_dir = './1219_DailyDialogue_embeddingmodel_CNN/ATT/GRU'\n",
    "log_dir_model = './result_logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 278)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 278, 50)       1180500     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 278, 50)       0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 278, 32)       1632        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)                (None, 278, 32)       3232        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, 278, 32)       128         conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 278, 32)       128         conv1d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 278, 32)       0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 278, 32)       0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 278, 64)       0           activation_10[0][0]              \n",
      "                                                                   activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 278, 128)      6528        activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)               (None, 278, 128)      8320        concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, 278, 128)      512         conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 278, 128)      512         conv1d_12[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 278, 128)      0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 278, 128)      0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 278, 128)      0           activation_9[0][0]               \n",
      "                                                                   activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 278, 128)      0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 278, 1)        129         dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 278)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 128, 278)      0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128, 278)      77562       repeat_vector_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attention_mul (Dot)              (None, 128, 128)      0           dense_2[0][0]                    \n",
      "                                                                   dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lstm (GRU)                       (None, 128, 128)      98688       attention_mul[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 16384)         0           lstm[0][0]                       \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           2097280     flatten_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 3,475,151\n",
      "Trainable params: 2,294,011\n",
      "Non-trainable params: 1,181,140\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuesiyuan/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:25: UserWarning: Output \"utterance_model\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"utterance_model\" during training.\n",
      "/home/xuesiyuan/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/xuesiyuan/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:31: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(steps_per_epoch=100, validation_data=<generator..., validation_steps=100, callbacks=[<__main__..., generator=<generator..., epochs=1000)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 1.0090updata loss from 10.0000 to 1.0090, save utterance_model\n",
      "updata loss from 10.0000 to 1.0000, save triplet_model\n",
      "100/100 [==============================] - 462s - loss: 1.0090 - val_loss: 1.0000\n",
      "Epoch 2/1000\n",
      " 99/100 [============================>.] - ETA: 3s - loss: 1.0000updata loss from 1.0090 to 1.0000, save utterance_model\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 3/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 4/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 5/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 6/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 7/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 8/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 9/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 10/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 11/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 12/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 13/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 14/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 15/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 16/1000\n",
      "100/100 [==============================] - 460s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 17/1000\n",
      "100/100 [==============================] - 460s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 18/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 19/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 20/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 21/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 22/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 23/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 24/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 25/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 26/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 27/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 28/1000\n",
      "100/100 [==============================] - 457s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 29/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 30/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 31/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 32/1000\n",
      "100/100 [==============================] - 457s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 33/1000\n",
      "100/100 [==============================] - 459s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 34/1000\n",
      "100/100 [==============================] - 457s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 35/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 36/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 37/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 38/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 39/1000\n",
      "100/100 [==============================] - 460s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 40/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 41/1000\n",
      "100/100 [==============================] - 457s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 42/1000\n",
      "100/100 [==============================] - 458s - loss: 1.0000 - val_loss: 1.0000\n",
      "Epoch 43/1000\n",
      " 47/100 [=============>................] - ETA: 180s - loss: 1.0000"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "class SaveSubModel(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = 10\n",
    "        self.val_loss = 10\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if self.losses > logs.get('loss'):\n",
    "            print ('updata loss from %.4f to %.4f, save utterance_model' % (self.losses, logs.get('loss')))\n",
    "            self.losses = logs.get('loss')\n",
    "            utterance_model.save(utterance_path )\n",
    "            \n",
    "        if self.val_loss > logs.get('val_loss'):\n",
    "            print ('updata loss from %.4f to %.4f, save triplet_model' % (self.val_loss, logs.get('val_loss')))\n",
    "            self.val_loss = logs.get('val_loss')\n",
    "            triplet_model.save(triplet_path)\n",
    "\n",
    "\n",
    "save_model = SaveSubModel()\n",
    "optimizer = optimizers.Adam(lr=0.001, decay=0.0001, clipnorm=1.0)#\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100)\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=0,write_graph=True, write_images=False)\n",
    "\n",
    "\n",
    "utterance_model,triplet_model = train_model()\n",
    "triplet_model.compile(loss = None, optimizer = optimizer)\n",
    "history = triplet_model.fit_generator(generator = generator(train,batch_size),\n",
    "                                      nb_epoch=1000,\n",
    "                                      steps_per_epoch = 100,\n",
    "                                      validation_data = generator(test,batch_size),\n",
    "                                      validation_steps= 100,\n",
    "                                      callbacks = [save_model,early_stopping,tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
